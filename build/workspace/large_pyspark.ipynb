{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PySpark**: The Apache Spark Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook shows how to connect Jupyter notebooks to a Spark cluster to process data using Spark Python API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Connection\n",
    "\n",
    "To connect to the Spark cluster, create a SparkSession object with the following params:\n",
    "\n",
    "+ **appName:** application name displayed at the [Spark Master Web UI](http://localhost:8080/);\n",
    "+ **master:** Spark Master URL, same used by Spark Workers;\n",
    "+ **spark.executor.memory:** must be less than or equals to docker compose SPARK_WORKER_MEMORY config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/16 08:50:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More confs for SparkSession object in standalone mode can be added using the **config** method. Checkout the API docs [here](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Introduction\n",
    "\n",
    "We will be using Spark Python API to read, process and write data. Checkout the API docs [here](https://spark.apache.org/docs/latest/api/python/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Read\n",
    "\n",
    "Let's read the data concerning Biomass energy production ([source](https://www.kaggle.com/datasets/parisrohan/credit-score-classification)) from the cluster's simulated **Spark standalone cluster** into a Spark dataframe.\n",
    "This dataset shows multiple information related to the details of more than 2 millions biomass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(path=\"data/generated_2millions_data.csv\", sep=\",\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then display some dataframe metadata, such as the number of rows and cols and its schema (cols name and type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2050118"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let s see the types of each column in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ID', 'string'),\n",
       " ('name', 'string'),\n",
       " ('Moisture content', 'string'),\n",
       " ('Volatile matter', 'string'),\n",
       " ('Fixed carbon', 'string'),\n",
       " ('Carbon', 'string'),\n",
       " ('Hydrogen', 'string'),\n",
       " ('Net calorific value (LHV)', 'string')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that all columns are \"string\" type. It is necessary to change some of the columns into \"float\" before proceding to modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns below are supposed to be numbers.\n",
    "\n",
    "We ll create a function that converts the selected columns into \"float\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Moisture content', 'Volatile matter', 'Fixed carbon', 'Carbon', 'Hydrogen', 'Net calorific value (LHV)']\n"
     ]
    }
   ],
   "source": [
    "columns = data.columns[2:8]\n",
    "print(columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- Moisture content: float (nullable = true)\n",
      " |-- Volatile matter: float (nullable = true)\n",
      " |-- Fixed carbon: float (nullable = true)\n",
      " |-- Carbon: float (nullable = true)\n",
      " |-- Hydrogen: float (nullable = true)\n",
      " |-- Net calorific value (LHV): float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def convert_to_float(df, column):\n",
    "    return df.withColumn(column, col(column).cast(\"float\"))\n",
    "\n",
    "# Apply the conversion to each column\n",
    "for column in columns:\n",
    "    data = convert_to_float(data, column)\n",
    "\n",
    "# Show the schema to verify the changes\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let s check the usual statistiques of our columns of the type \"float\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+------------------+-------------------------+\n",
      "|summary|  Moisture content|  Volatile matter|      Fixed carbon|            Carbon|          Hydrogen|Net calorific value (LHV)|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+-------------------------+\n",
      "|  count|           2050118|          2050118|           2050118|           2050118|           2050118|                  2050118|\n",
      "|   mean| 21.59959719049037|56.61321345239012| 19.04222656823365|40.768696074712544|4.6419355674140945|       14.847574745704133|\n",
      "| stddev|15.321312125080546|20.37650947685937|12.204060212590859| 15.11232645800606|1.7320383906153491|        6.994227433671838|\n",
      "|    min|     1.09942885E-5|     3.6880007E-5|      6.7279407E-6|      5.0687526E-5|      2.7098176E-5|                -9.005806|\n",
      "|    max|         115.54109|        157.13931|          90.92725|         113.17256|         13.268918|                 50.16783|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+-------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "summary_stats = data[columns].describe()\n",
    "summary_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Interpretation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the table above, we can find many unusual values.\n",
    "\n",
    "For example:\n",
    "the mean of the column \"Age\" is 124.25.\n",
    "\n",
    "the mean of the columns \"Num_Bank_Accounts\" is 16.57.\n",
    "\n",
    "there are many values that seems much bigger than they should be, and that is due to mistyping and incorrect data that could ve been wrongly registered for various reasons.\n",
    "\n",
    "<h5>Our job is to fix this data before creating a model</h5>\n",
    "Let s create a boxplot for each columns to check the distrubution of values in each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "negative_values_count = data.filter(col('Net calorific value (LHV)') < 0).count()\n",
    "print(negative_values_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let s remove the rows where the \"net calorific value\" is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------+\n",
      "|summary|Net calorific value (LHV)|\n",
      "+-------+-------------------------+\n",
      "|  count|                  2015864|\n",
      "|   mean|       15.131477251125487|\n",
      "| stddev|        6.700126756281459|\n",
      "|    min|             1.7995875E-4|\n",
      "|    max|                 50.16783|\n",
      "+-------+-------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = data.filter(col('Net calorific value (LHV)') >= 0)\n",
    "net_cal_val_stats = data.describe(\"Net calorific value (LHV)\")\n",
    "net_cal_val_stats.show()\n",
    "print(data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our data is cleaned (at the cost of 30 000 rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Correlation matrix</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 91:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between Moisture content and Volatile matter: -0.0017847007091230658\n",
      "Correlation between Moisture content and Fixed carbon: -0.0009742749881004561\n",
      "Correlation between Moisture content and Carbon: 0.005826871260981072\n",
      "Correlation between Moisture content and Hydrogen: 0.001056513892655901\n",
      "Correlation between Moisture content and Net calorific value (LHV): -0.05361232675553769\n",
      "Correlation between Volatile matter and Fixed carbon: -0.001462396461517261\n",
      "Correlation between Volatile matter and Carbon: 0.01585810873317755\n",
      "Correlation between Volatile matter and Hydrogen: 0.0018277480245005278\n",
      "Correlation between Volatile matter and Net calorific value (LHV): -0.1577507345643554\n",
      "Correlation between Fixed carbon and Carbon: 0.014205582625720905\n",
      "Correlation between Fixed carbon and Hydrogen: -0.0007166576828273393\n",
      "Correlation between Fixed carbon and Net calorific value (LHV): -0.11096182610809648\n",
      "Correlation between Carbon and Hydrogen: -0.008061316044390929\n",
      "Correlation between Carbon and Net calorific value (LHV): 0.9716325693999551\n",
      "Correlation between Hydrogen and Net calorific value (LHV): 0.07386003036333662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "from itertools import combinations\n",
    "\n",
    "# Create an empty dictionary to store the correlation values\n",
    "correlation_dict = {}\n",
    "\n",
    "# Use combinations to get unique pairs of columns\n",
    "for col1, col2 in combinations(columns, 2):\n",
    "    correlation = data.stat.corr(col1, col2)\n",
    "    correlation_dict[(col1, col2)] = correlation\n",
    "\n",
    "# Display the correlation matrix\n",
    "for key, value in correlation_dict.items():\n",
    "    print(f\"Correlation between {key[0]} and {key[1]}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
