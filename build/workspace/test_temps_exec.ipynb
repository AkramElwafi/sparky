{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PySpark**: Test de temps d'execution (start docker spark standalone cluster in /build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/21 14:37:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/21 14:37:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/07/21 14:38:08 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/07/21 14:38:26 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:38:41 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:38:56 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:39:11 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:39:26 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:39:41 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:39:56 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:40:11 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:40:26 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:40:41 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:40:56 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:41:11 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:41:26 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:41:41 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:41:56 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:42:11 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:42:26 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:42:41 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:42:56 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:43:11 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:43:26 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:43:41 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:43:56 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:44:11 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:44:26 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:44:41 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:44:56 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:45:11 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:45:26 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:45:41 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:45:56 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/07/21 14:46:11 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "[Stage 0:>                                                          (0 + 0) / 2]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import corr\n",
    "from itertools import combinations\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField(\"biomass_id\", FloatType(), True),\n",
    "    StructField(\"biomass_name\", StringType(), True),\n",
    "    StructField(\"Moisture content\", FloatType(), True),\n",
    "    StructField(\"Volatile matter\", FloatType(), True),\n",
    "    StructField(\"Fixed carbon\", FloatType(), True),\n",
    "    StructField(\"Carbon\", FloatType(), True),\n",
    "    StructField(\"Hydrogen\", FloatType(), True),\n",
    "    StructField(\"Net calorific value (LHV)\", FloatType(), True),\n",
    "])\n",
    "\n",
    "input_path = 'data/generated_2mill_data.csv'\n",
    "df = spark.read.csv(input_path, header=True, schema=custom_schema)\n",
    "\n",
    "df = df.drop('biomass_id', 'biomass_name')\n",
    "\n",
    "columns = df.columns\n",
    "\n",
    "# Calculate statistics\n",
    "stats = df.agg(\n",
    "    *[F.mean(col).alias(f'Mean_{col}') for col in columns],\n",
    "    *[F.stddev(col).alias(f'StdDev_{col}') for col in columns],\n",
    "    *[F.min(col).alias(f'Min_{col}') for col in columns],\n",
    "    *[F.max(col).alias(f'Max_{col}') for col in columns],\n",
    "    *[F.expr(f'percentile_approx(`{col}`, 0.25)').alias(f'25th_Percentile_{col}') for col in columns],\n",
    "    *[F.expr(f'percentile_approx(`{col}`, 0.75)').alias(f'75th_Percentile_{col}') for col in columns],\n",
    "    *[F.kurtosis(col).alias(f'Kurtosis_{col}') for col in columns],\n",
    "    *[F.skewness(col).alias(f'Skewness_{col}') for col in columns]\n",
    ")\n",
    "\n",
    "stats.show()\n",
    "windowSpec = Window.rowsBetween(-10000, 0)\n",
    "expandingWindowSpec = Window.orderBy(\"Moisture content\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "orderedWindowSpec = Window.orderBy(\"Moisture content\")\n",
    "\n",
    "for col_name in columns:\n",
    "    for _ in range(10):\n",
    "        df = df.withColumn(f'{col_name}_rolling_mean', F.avg(col(col_name)).over(windowSpec))\n",
    "        df = df.withColumn(f'{col_name}_rolling_std', F.stddev(col(col_name)).over(windowSpec))\n",
    "        df = df.withColumn(f'{col_name}_expanding_mean', F.avg(col(col_name)).over(expandingWindowSpec))\n",
    "        df = df.withColumn(f'{col_name}_expanding_std', F.stddev(col(col_name)).over(expandingWindowSpec))\n",
    "\n",
    "for col_name in columns:\n",
    "    for _ in range(10):\n",
    "        df = df.withColumn(f'{col_name}_log', F.log1p(col(col_name)))\n",
    "        df = df.withColumn(f'{col_name}_sqrt', F.sqrt(col(col_name)))\n",
    "        df = df.withColumn(f'{col_name}_cumsum', F.sum(col(col_name)).over(expandingWindowSpec))\n",
    "        df = df.withColumn(f'{col_name}_cumprod', F.exp(F.sum(F.log(col(col_name) + 1)).over(expandingWindowSpec)))\n",
    "        df = df.withColumn(f'{col_name}_exp', F.exp(col(col_name)))\n",
    "        df = df.withColumn(f'{col_name}_sin', F.sin(col(col_name)))\n",
    "        df = df.withColumn(f'{col_name}_cos', F.cos(col(col_name)))\n",
    "        df = df.withColumn(f'{col_name}_tan', F.tan(col(col_name)))\n",
    "        df = df.withColumn(f'{col_name}_diff', col(col_name) - F.lag(col(col_name), 1).over(orderedWindowSpec))\n",
    "        df = df.withColumn(f'{col_name}_pct_change', (col(col_name) - F.lag(col(col_name), 1).over(orderedWindowSpec)) / F.lag(col(col_name), 1).over(orderedWindowSpec))\n",
    "\n",
    "grouped_stats = df.groupBy((F.floor(F.monotonically_increasing_id() / 100)).alias(\"group\")).agg(\n",
    "    F.mean('Moisture content').alias('Mean_Moisture content'),\n",
    "    F.stddev('Moisture content').alias('StdDev_Moisture content'),\n",
    "    F.min('Moisture content').alias('Min_Moisture content'),\n",
    "    F.max('Moisture content').alias('Max_Moisture content'),\n",
    "    F.mean('Volatile matter').alias('Mean_Volatile matter'),\n",
    "    F.stddev('Volatile matter').alias('StdDev_Volatile matter'),\n",
    "    F.min('Volatile matter').alias('Min_Volatile matter'),\n",
    "    F.max('Volatile matter').alias('Max_Volatile matter'),\n",
    "    F.mean('Fixed carbon').alias('Mean_Fixed carbon'),\n",
    "    F.stddev('Fixed carbon').alias('StdDev_Fixed carbon'),\n",
    "    F.min('Fixed carbon').alias('Min_Fixed carbon'),\n",
    "    F.max('Fixed carbon').alias('Max_Fixed carbon'),\n",
    "    F.mean('Carbon').alias('Mean_Carbon'),\n",
    "    F.stddev('Carbon').alias('StdDev_Carbon'),\n",
    "    F.min('Carbon').alias('Min_Carbon'),\n",
    "    F.max('Carbon').alias('Max_Carbon'),\n",
    "    F.mean('Hydrogen').alias('Mean_Hydrogen'),\n",
    "    F.stddev('Hydrogen').alias('StdDev_Hydrogen'),\n",
    "    F.min('Hydrogen').alias('Min_Hydrogen'),\n",
    "    F.max('Hydrogen').alias('Max_Hydrogen'),\n",
    "    F.mean('Net calorific value (LHV)').alias('Mean_Net calorific value (LHV)'),\n",
    "    F.stddev('Net calorific value (LHV)').alias('StdDev_Net calorific value (LHV)'),\n",
    "    F.min('Net calorific value (LHV)').alias('Min_Net calorific value (LHV)'),\n",
    "    F.max('Net calorific value (LHV)').alias('Max_Net calorific value (LHV)')\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Total execution time: {end_time - start_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
